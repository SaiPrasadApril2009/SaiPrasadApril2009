{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Launch the web UI\n",
        "\n",
        "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "os.environ.pop('MPLBACKEND', None)\n",
        "\n",
        "if Path.cwd().name != 'text-generation-webui':\n",
        "  print(\"\\033[1;32;1m\\n --> Installing the web UI. This will take a while, but after the initial setup, you can download and test as many models as you like.\\033[0;37;0m\\n\")\n",
        "\n",
        "  !git clone https://github.com/oobabooga/text-generation-webui\n",
        "  %cd text-generation-webui\n",
        "\n",
        "  # Install the project in an isolated environment\n",
        "  !GPU_CHOICE=A \\\n",
        "  USE_CUDA118=FALSE \\\n",
        "  LAUNCH_AFTER_INSTALL=FALSE \\\n",
        "  INSTALL_EXTENSIONS=FALSE \\\n",
        "  ./start_linux.sh\n",
        "\n",
        "# Parameters\n",
        "model_url = \"https://huggingface.co/turboderp/gemma-2-9b-it-exl2\" #@param {type:\"string\"}\n",
        "branch = \"8.0bpw\" #@param {type:\"string\"}\n",
        "command_line_flags = \"--n-gpu-layers 128 --load-in-4bit --use_double_quant --no_flash_attn\" #@param {type:\"string\"}\n",
        "api = True #@param {type:\"boolean\"}\n",
        "\n",
        "if api:\n",
        "  for param in ['--api', '--public-api']:\n",
        "    if param not in command_line_flags:\n",
        "      command_line_flags += f\" {param}\"\n",
        "\n",
        "model_url = model_url.strip()\n",
        "if model_url != \"\":\n",
        "    if not model_url.startswith('http'):\n",
        "        model_url = 'https://huggingface.co/' + model_url\n",
        "\n",
        "    # Download the model\n",
        "    url_parts = model_url.strip('/').strip().split('/')\n",
        "    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
        "    branch = branch.strip('\"\\' ')\n",
        "    if branch.strip() not in ['', 'main']:\n",
        "        output_folder += f\"_{branch}\"\n",
        "        !python download-model.py {model_url} --branch {branch}\n",
        "    else:\n",
        "        !python download-model.py {model_url}\n",
        "else:\n",
        "    output_folder = \"\"\n",
        "\n",
        "# Start the web UI\n",
        "cmd = f\"./start_linux.sh {command_line_flags} --share\"\n",
        "if output_folder != \"\":\n",
        "    cmd += f\" --model {output_folder}\"\n",
        "\n",
        "!$cmd"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Hello\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "# # oobabooga/text-generation-webui\n",
        "#\n",
        "# After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "#\n",
        "# * Project page: https://github.com/oobabooga/text-generation-webui\n",
        "# * Gradio server status: https://status.gradio.app/\n",
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n",
        "#@title 2. Launch the web UI\n",
        "\n",
        "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
        "\n",
        "\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "os.environ.pop('MPLBACKEND', None)\n",
        "\n",
        "if Path.cwd().name != 'text-generation-webui':\n",
        "  print(\"\\033[1;32;1m\\n --> Installing the web UI. This will take a while, but after the initial setup, you can download and test as many models as you like.\\033[0;37;0m\\n\")\n",
        "\n",
        "  !git clone https://github.com/oobabooga/text-generation-webui\n",
        "  %cd text-generation-webui\n",
        "\n",
        "  # Install the project in an isolated environment\n",
        "  !GPU_CHOICE=A \\\n",
        "  USE_CUDA118=FALSE \\\n",
        "  LAUNCH_AFTER_INSTALL=FALSE \\\n",
        "  INSTALL_EXTENSIONS=FALSE \\\n",
        "  ./start_linux.sh\n",
        "\n",
        "# Parameters\n",
        "model_url = \"https://huggingface.co/turboderp/gemma-2-9b-it-exl2\" #@param {type:\"string\"}\n",
        "branch = \"8.0bpw\" #@param {type:\"string\"}\n",
        "command_line_flags = \"--n-gpu-layers 128 --load-in-4bit --use_double_quant --no_flash_attn\" #@param {type:\"string\"}\n",
        "api = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Added input field for custom command line flags\n",
        "custom_command_line_flags = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if api:\n",
        "  for param in ['--api', '--public-api']:\n",
        "    if param not in command_line_flags:\n",
        "      command_line_flags += f\" {param}\"\n",
        "\n",
        "model_url = model_url.strip()\n",
        "if model_url != \"\":\n",
        "    if not model_url.startswith('http'):\n",
        "        model_url = 'https://huggingface.co/' + model_url\n",
        "\n",
        "    # Download the model\n",
        "    url_parts = model_url.strip('/').strip().split('/')\n",
        "    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
        "    branch = branch.strip('\"\\' ')\n",
        "    if branch.strip() not in ['', 'main']:\n",
        "        output_folder += f\"_{branch}\"\n",
        "        !python download-model.py {model_url} --branch {branch}\n",
        "    else:\n",
        "        !python download-model.py {model_url}\n",
        "else:\n",
        "    output_folder = \"\"\n",
        "\n",
        "# Combine command line flags\n",
        "command_line_flags += \" \" + custom_command_line_flags\n",
        "\n",
        "# Start the web UI\n",
        "cmd = f\"./start_linux.sh {command_line_flags} --share\"\n",
        "if output_folder != \"\":\n",
        "    cmd += f\" --model {output_folder}\"\n",
        "\n",
        "!$cmd\n"
      ],
      "metadata": {
        "id": "nMfHtBrYSgmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Task\n",
        "Tell me about this dataset.\n",
        "\n",
        "Here is all the data you need:\n",
        "\"sai_style.txt\""
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ueX98QBAlWsD"
      }
    },
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the text file \"sai_style.txt\" into a string variable.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "TpaBxbIalW-X"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "I need to read the content of the file \"sai_style.txt\" into a string variable.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "7GQBFqLylXPe"
      }
    },
    {
      "source": [
        "try:\n",
        "    with open(\"sai_style.txt\", \"r\") as file:\n",
        "        file_content = file.read()\n",
        "except FileNotFoundError:\n",
        "    file_content = None\n",
        "    print(\"Error: 'sai_style.txt' not found.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dPhtebK0lXfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded text data to understand its basic characteristics.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBicp7PlaLG"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Analyze the text data in `file_content` to understand its characteristics, including length, character types, word count, sentence count, and unusual patterns.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "uKacMqvQlbDl"
      }
    },
    {
      "source": [
        "import re\n",
        "\n",
        "if file_content:\n",
        "    string_length = len(file_content)\n",
        "\n",
        "    alphanumeric_count = sum(1 for char in file_content if char.isalnum())\n",
        "    punctuation_count = sum(1 for char in file_content if char in \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\")\n",
        "    whitespace_count = sum(1 for char in file_content if char.isspace())\n",
        "    other_count = string_length - alphanumeric_count - punctuation_count - whitespace_count\n",
        "\n",
        "    alphanumeric_proportion = alphanumeric_count / string_length if string_length else 0\n",
        "    punctuation_proportion = punctuation_count / string_length if string_length else 0\n",
        "    whitespace_proportion = whitespace_count / string_length if string_length else 0\n",
        "    other_proportion = other_count / string_length if string_length else 0\n",
        "\n",
        "    word_count = len(file_content.split())\n",
        "\n",
        "    sentence_count = len(re.findall(r'[.!?]', file_content))\n",
        "\n",
        "    unusual_characters = set()\n",
        "    for char in file_content:\n",
        "        if not char.isalnum() and not char.isspace() and char not in \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\":\n",
        "            unusual_characters.add(char)\n",
        "\n",
        "    print(f\"String Length: {string_length}\")\n",
        "    print(f\"Alphanumeric Proportion: {alphanumeric_proportion:.2f}\")\n",
        "    print(f\"Punctuation Proportion: {punctuation_proportion:.2f}\")\n",
        "    print(f\"Whitespace Proportion: {whitespace_proportion:.2f}\")\n",
        "    print(f\"Other Proportion: {other_proportion:.2f}\")\n",
        "    print(f\"Word Count: {word_count}\")\n",
        "    print(f\"Sentence Count (estimated): {sentence_count}\")\n",
        "    print(f\"Unusual Characters: {unusual_characters}\")\n",
        "else:\n",
        "    print(\"The file could not be loaded, so the analysis cannot be performed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mTIa4iVElbUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the loaded text data to gain insights into its content.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "148LohoklehI"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Perform word frequency analysis, identify themes, assess tone and style, and identify recurring patterns in the text.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "QIbeXM4ylfX8"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "if file_content:\n",
        "    # Tokenize the text and remove punctuation\n",
        "    tokens = word_tokenize(file_content)\n",
        "    words = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [w for w in words if not w in stop_words]\n",
        "\n",
        "    # Word frequency analysis\n",
        "    word_counts = Counter(filtered_words)\n",
        "    most_common_words = word_counts.most_common(10)  # Get the 10 most common words\n",
        "\n",
        "    print(\"Most common words:\", most_common_words)\n",
        "\n",
        "    # Analyze themes (example: group words related to technology)\n",
        "    technology_words = [word for word in filtered_words if word in [\"technology\", \"computer\", \"software\", \"internet\", \"code\", \"programming\"]]\n",
        "    print(\"\\nTechnology related words:\", technology_words)\n",
        "\n",
        "\n",
        "    # Assess tone and style (example: count exclamation marks)\n",
        "    exclamation_count = file_content.count(\"!\")\n",
        "    print(f\"\\nExclamation count: {exclamation_count}\")\n",
        "    if exclamation_count > 5:\n",
        "        print(\"The text appears to have an enthusiastic tone.\")\n",
        "    else:\n",
        "        print(\"The text does not seem very enthusiastic.\")\n",
        "\n",
        "    # Identify recurring phrases (example: find all occurrences of \"very good\")\n",
        "    recurring_phrases = re.findall(r\"very good\", file_content, re.IGNORECASE)\n",
        "    print(f\"\\nOccurrences of 'very good': {len(recurring_phrases)}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"The file could not be loaded, so the analysis cannot be performed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cyD0lv9Ylfn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to a missing NLTK resource.  I need to download the necessary resource `punkt_tab` and then retry the code.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "WuopO0OAljP0"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "if file_content:\n",
        "    # Tokenize the text and remove punctuation\n",
        "    tokens = word_tokenize(file_content)\n",
        "    words = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [w for w in words if not w in stop_words]\n",
        "\n",
        "    # Word frequency analysis\n",
        "    word_counts = Counter(filtered_words)\n",
        "    most_common_words = word_counts.most_common(10)  # Get the 10 most common words\n",
        "\n",
        "    print(\"Most common words:\", most_common_words)\n",
        "\n",
        "    # Analyze themes (example: group words related to technology)\n",
        "    technology_words = [word for word in filtered_words if word in [\"technology\", \"computer\", \"software\", \"internet\", \"code\", \"programming\"]]\n",
        "    print(\"\\nTechnology related words:\", technology_words)\n",
        "\n",
        "\n",
        "    # Assess tone and style (example: count exclamation marks)\n",
        "    exclamation_count = file_content.count(\"!\")\n",
        "    print(f\"\\nExclamation count: {exclamation_count}\")\n",
        "    if exclamation_count > 5:\n",
        "        print(\"The text appears to have an enthusiastic tone.\")\n",
        "    else:\n",
        "        print(\"The text does not seem very enthusiastic.\")\n",
        "\n",
        "    # Identify recurring phrases (example: find all occurrences of \"very good\")\n",
        "    recurring_phrases = re.findall(r\"very good\", file_content, re.IGNORECASE)\n",
        "    print(f\"\\nOccurrences of 'very good': {len(recurring_phrases)}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"The file could not be loaded, so the analysis cannot be performed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QneobMAaljfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Summary:\n",
        "\n",
        "### 1. Q&A\n",
        "\n",
        "The provided code does not explicitly pose questions. However, the analysis implicitly aims to answer questions like:\n",
        "\n",
        "* What are the most frequent words in the text?\n",
        "* Are there any recurring themes or topics?\n",
        "* What is the overall tone of the text?\n",
        "* Are there any frequently occurring phrases?\n",
        "\n",
        "\n",
        "### 2. Data Analysis Key Findings\n",
        "\n",
        "* **Top 10 Frequent Words:**  After removing stop words, the most frequent words are 'like'(7), 'vibes'(5), 'sai'(4), 'okay'(4), 'bro'(4), 'ai'(3), 'say'(3), 'code'(3), 'one'(3), and 'start'(2).\n",
        "* **Technology Theme:** The words \"internet\" and \"code\" appear at least three times, suggesting a potential technology theme.\n",
        "* **Neutral Tone:** The text has an exclamation count of 0, indicating a neutral or non-enthusiastic tone.\n",
        "* **Absence of \"very good\":** The phrase \"very good\" does not appear in the text.\n",
        "* **Unusual Characters:** The text contains several unusual characters, including emojis and special symbols, suggesting informal or casual writing style.\n",
        "\n",
        "\n",
        "### 3. Insights or Next Steps\n",
        "\n",
        "* **Investigate \"sai\" and \"vibes\":** The high frequency of \"sai\" and \"vibes\" suggests they may be central to the text's meaning. Further investigation into their context could reveal deeper insights.\n",
        "* **Expand Theme Analysis:**  Explore other potential themes beyond technology, such as those related to gaming, social interaction, or personal experiences, given the presence of informal language and emojis.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ApRHqNCwlnTb"
      }
    }
  ]
}